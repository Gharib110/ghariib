---
title: "Malicious ML models discovered on Hugging Face platform"
date: Thu, 06 Feb 2025 16:00:02 GMT
draft: false
type: posts
categories: 
- Threat Research
---
# Malicious ML models discovered on Hugging Face platform

<br/>

<br/>
[![RL identifies malware in Hugging Face ML model](https://www.reversinglabs.com/hubfs/Blog/hugging-face-blog.webp)](https://www.reversinglabs.com/blog/rl-identifies-malware-ml-model-hosted-on-hugging-face)

In the last few months, artificial intelligence (AI) is popping up in all kinds of headlines, ranging from technical software developer websites to the Sunday comics. There’s no secret why. Given the recent explosion in the capabilities of large language models (LLMs) and generative AI, organizations are trying to find ways to incorporate AI technologies into their business models — and to make use of its capabilities.   
  
While most non-technical people think of OpenAI’s ChatGPT when AI is mentioned (or maybe its Chinese competitor DeepSeek), developers and others familiar with machine learning (ML) models and the technology that supports AI will likely think of Hugging Face, a platform dedicated to collaboration and sharing of ML projects. As described in its organization card on the Hugging Face platform, the company is “on a mission to democratize good machine learning.”   
  
That democratization is happening. But with AI’s growing popularity and use, platforms like Hugging Face are now being targeted by threat actors, who are seeking new, hard-to-detect ways of inserting and distributing malicious software to unsuspecting hosts.   
  
This was underscored by a discovery the ReversingLabs research team made on the Hugging Face platform: a novel technique of distributing malware by [abusing Pickle file serialization](https://www.reversinglabs.com/blog/spectra-assure-malware-detection-in-ml-and-llm-models) — a not-so-novel exploitation target.

#### [Source](https://www.reversinglabs.com/blog/rl-identifies-malware-ml-model-hosted-on-hugging-face)

<br/>
---
